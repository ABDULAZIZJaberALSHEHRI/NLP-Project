{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIuJtTjKVMXs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7UA9RjoOfhZ"
      },
      "source": [
        "#  Step 1: Import the necessary  library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "84OBDAQ4Ofhc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3X70SxpOfhe"
      },
      "source": [
        "# Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vYVTfJaOfhe",
        "outputId": "784ccfb2-eac0-4fba-d4e5-ce963ecbea99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Loaded. Shape: (39942, 5)\n",
            "\n",
            "Missing Values:\n",
            " label      0\n",
            "title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "print(\"Dataset Loaded. Shape:\", df.shape)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4N7OZc6QOfhf"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing Function\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQoHhFTUOfhg",
        "outputId": "b5c15133-869b-4eb8-80e5-9d384d6352f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample cleaned text:\n",
            " 0    washington reuters the head of a conservative ...\n",
            "1    washington reuters transgender people will be ...\n",
            "2    washington reuters the special counsel investi...\n",
            "3    washington reuters trump campaign adviser geor...\n",
            "4    seattle washington reuters president donald tr...\n",
            "Name: clean_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Apply text cleaning to the \"text\" column\n",
        "df['clean_text'] = df['text'].map(clean_text)\n",
        "\n",
        "df['clean_title'] = df['title'].map(clean_text)\n",
        "\n",
        "df[\"combined_text\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "# Confirm cleaning worked\n",
        "\n",
        "# Confirm cleaning worked\n",
        "print(\"\\nSample cleaned text:\\n\", df['clean_text'].head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tjF0JWIS1B6",
        "outputId": "a5e443b7-1755-41ad-f21f-03cd8020e49e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34p4eiFdOfhg"
      },
      "source": [
        "# Step 3: Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmSyN-WDOfhh",
        "outputId": "7c14bb9d-24d2-47d7-e9b4-2d5af108d484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚è±Ô∏è Model training execution time: 22.148958 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "# Split the dataset (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['combined_text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# print(\"Training Set Size:\", len(X_train))\n",
        "# print(\"Testing Set Size:\", len(X_test))\n",
        "\n",
        "custom_stopwords = list(set(stopwords.words('english')))\n",
        "# Apply TF-IDF (Limit to 5000 most important words)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=custom_stopwords)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['text'])\n",
        "\n",
        "words = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "# print(\"TF-IDF Shape:\", X_tfidf.shape)\n",
        "\n",
        "\n",
        "# Fit TF-IDF only on training data & transform both train & test\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# print(\"\\nTF-IDF Applied:\")\n",
        "# print(\"Train Shape:\", X_train_tfidf.shape)\n",
        "# print(\"Test Shape:\", X_test_tfidf.shape)\n",
        "\n",
        "# Convert sparse matrices to dense NumPy arrays\n",
        "X_train_dense = X_train_tfidf.toarray().astype(np.float64)\n",
        "X_test_dense = X_test_tfidf.toarray().astype(np.float64)\n",
        "\n",
        "# Convert labels to NumPy array\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Confirm everything is correctly formatted\n",
        "# print(\"\\nData Types & Shapes:\")\n",
        "# print(\"Train Data Type:\", type(X_train_dense))\n",
        "# print(\"Train Data Shape:\", X_train_dense.shape)\n",
        "# print(\"Test Data Type:\", type(X_test_dense))\n",
        "# print(\"Test Data Shape:\", X_test_dense.shape)\n",
        "\n",
        "# End timing\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print execution time\n",
        "print(f\"\\n‚è±Ô∏è Model training execution time: {elapsed_time:.6f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LayZCXHVOfhh",
        "outputId": "984369a4-5d44-4fd5-89f8-fbab1d7ac025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Random Forest Model Trained Successfully!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the Random Forest Model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=10,\n",
        "    random_state=42)\n",
        "rf_model.fit(X_train_dense, y_train)\n",
        "\n",
        "print(\" Random Forest Model Trained Successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcjOSJruOfhh"
      },
      "source": [
        "#  Step 4: Evaluate the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vJDdbK9TOfhi",
        "outputId": "592a7bab-8cd9-4fb2-e6d2-2f31c5519be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Random Forest Accuracy: 0.9821\n",
            "\n",
            "üìä Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      3996\n",
            "           1       0.97      0.99      0.98      3993\n",
            "\n",
            "    accuracy                           0.98      7989\n",
            "   macro avg       0.98      0.98      0.98      7989\n",
            "weighted avg       0.98      0.98      0.98      7989\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make Predictions\n",
        "y_pred_rf = rf_model.predict(X_test_dense)\n",
        "\n",
        "# Evaluate the Model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "report_rf = classification_report(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"\\n Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
        "print(\"\\nüìä Classification Report:\\n\", report_rf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkPztFu5QMxP",
        "outputId": "1d81d243-5342-4107-a031-f47c5715436b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Accuracy: 0.9864\n",
            " Testing Accuracy: 0.9821\n"
          ]
        }
      ],
      "source": [
        "# Get accuracy on training data\n",
        "y_train_pred_rf = rf_model.predict(X_train_dense)\n",
        "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
        "\n",
        "print(f\" Training Accuracy: {train_accuracy_rf:.4f}\")\n",
        "print(f\" Testing Accuracy: {accuracy_rf:.4f}\")  # From previous step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ub-4pQQs5D",
        "outputId": "0da94411-1df9-41be-899b-6001a0f676ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Top 10 Most Important Words for Classification:\n",
            "['know', 'video', 'watch', 'getty', 'image', 'washington', 'featured', 'said', 'via', 'reuters']\n"
          ]
        }
      ],
      "source": [
        "# Get feature importance\n",
        "feature_importance = rf_model.feature_importances_\n",
        "\n",
        "# Get top 10 most important words\n",
        "top_n = 10\n",
        "indices = np.argsort(feature_importance)[-top_n:]\n",
        "top_words = [tfidf_vectorizer.get_feature_names_out()[i] for i in indices]\n",
        "\n",
        "print(\"üîπ Top 10 Most Important Words for Classification:\")\n",
        "print(top_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GyigvxtQzJf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(rf_model, X_train_dense, y_train, cv=5)\n",
        "print(f\"‚úÖ Cross-validation Accuracy:,  {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQOthOpqlqjX"
      },
      "source": [
        "# Some Graphs present our Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BuLm-DxvfTM"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rL39Y4qmD1_"
      },
      "outputs": [],
      "source": [
        "# Get probabilities\n",
        "y_probs_rf = rf_model.predict_proba(X_test_dense)[:,1]\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs_rf)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUPGl8IWmEV4"
      },
      "outputs": [],
      "source": [
        "# Get feature importance\n",
        "feature_importance = rf_model.feature_importances_\n",
        "\n",
        "# Get top 10 most important words\n",
        "top_n = 10\n",
        "indices = np.argsort(feature_importance)[-top_n:]\n",
        "top_words = [tfidf_vectorizer.get_feature_names_out()[i] for i in indices]\n",
        "top_values = feature_importance[indices]\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(top_words, top_values, color='royalblue')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Top Words')\n",
        "plt.title('Top 10 Most Important Words for Classification')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0iU5jpjJgFG"
      },
      "source": [
        "# comparison between Random Forest and SVM and Na√Øve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX7nQMw3Jfpc"
      },
      "outputs": [],
      "source": [
        "# Load trained models\n",
        "nb_model = joblib.load(\"naive_bayes_model.pkl\")\n",
        "rf_model = joblib.load(\"random_forest_model.pkl\")\n",
        "svm_model = joblib.load(\"svm_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFJ_7HuML0y2"
      },
      "outputs": [],
      "source": [
        "models = [\"Na√Øve Bayes\", \"Random Forest\", \"SVM\"]\n",
        "training_accuracy = [0.9168, 0.9675, 0.945]  # Replace with actual SVM training accuracy\n",
        "testing_accuracy = [0.9171, 0.9588, 0.937]   # Replace with actual SVM testing accuracy\n",
        "cv_accuracy = [0.9136, 0.9577, 0.934]       # Replace with actual SVM cross-validation accuracy\n",
        "\n",
        "# Bar Width\n",
        "bar_width = 0.25\n",
        "x_indexes = np.arange(len(models))\n",
        "\n",
        "# Create Bar Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(x_indexes, training_accuracy, width=bar_width, label=\"Training Accuracy\", color='#6a89cc')\n",
        "plt.bar(x_indexes + bar_width, testing_accuracy, width=bar_width, label=\"Testing Accuracy\", color='#82ccdd')\n",
        "plt.bar(x_indexes + 2 * bar_width, cv_accuracy, width=bar_width, label=\"Cross-Validation Accuracy\", color='#b8e994')\n",
        "\n",
        "# Labels & Legends\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.xticks(ticks=x_indexes + bar_width, labels=models)\n",
        "plt.legend()\n",
        "plt.ylim(0.85, 1.0)\n",
        "# Show Plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewj01nM5GU3k"
      },
      "source": [
        "# Step 5: Predict on validation_data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1MHKvIbGa3O"
      },
      "source": [
        "## Load and Preprocess the the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0de8uAAOGVUB"
      },
      "outputs": [],
      "source": [
        "# Load the validation dataset\n",
        "df_validation = pd.read_csv(\"validation_data_randomForest.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXCi2e5wGY05"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text (same cleaning function)\n",
        "df_validation['clean_text'] = df_validation['text'].map(clean_text)\n",
        "\n",
        "# Transform text using the trained TF-IDF vectorizer\n",
        "X_validation_tfidf = tfidf_vectorizer.transform(df_validation['clean_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l25QRzV_HPbw"
      },
      "source": [
        "# Step 6: Predict labels using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWQTZzfYHN52"
      },
      "outputs": [],
      "source": [
        "df_validation['label'] = rf_model.predict(X_validation_tfidf)\n",
        "# Ensure label \"2\" is replaced with \"0\" or \"1\"\n",
        "df_validation['label'] = df_validation['label'].replace(2, 0)  # Just in case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQHMMpmYHO2S"
      },
      "source": [
        "# Step 7: Save the updated validation file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XGfLlwnHcBK"
      },
      "outputs": [],
      "source": [
        "df_validation.to_csv(\"validation_data_RandomForest.csv\", index=False)\n",
        "\n",
        "print(\" Predictions saved to validation_data_RandomForest.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RzPaikqtkvD2",
        "outputId": "95096a9e-68ad-45bc-dab5-4f40dcb8ebfd"
      },
      "outputs": [
        {
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '\\x0c'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-32ac34a9f7d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÖŸÜ ÿßŸÑŸÖŸÑŸÅ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random_forest.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# ÿ≥ÿ¨ŸÑ ÿßŸÑŸàŸÇÿ™ ÿ®ÿπÿØ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x0c'."
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import time\n",
        "\n",
        "# ÿ≥ÿ¨ŸÑ ÿßŸÑŸàŸÇÿ™ ŸÇÿ®ŸÑ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\n",
        "start_time = time.time()\n",
        "\n",
        "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÖŸÜ ÿßŸÑŸÖŸÑŸÅ\n",
        "with open(\"random_forest.pkl\", \"rb\") as file:\n",
        "  rf_model = pickle.load(file)\n",
        "\n",
        "# ÿ≥ÿ¨ŸÑ ÿßŸÑŸàŸÇÿ™ ÿ®ÿπÿØ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\n",
        "end_time = time.time()\n",
        "\n",
        "# ÿ≠ÿ≥ÿßÿ® ŸÖÿØÿ© ÿßŸÑÿ™ÿ≠ŸÖŸäŸÑ\n",
        "loading_time = end_time - start_time\n",
        "print(f\"üìÇ‚è≥ ŸàŸÇÿ™ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÖŸÜ `pkl`: {loading_time:.4f} ÿ´ÿßŸÜŸäÿ©\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrJFWIpek0rR",
        "outputId": "1da4539a-3a28-4602-f797-10ff6b105108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨: invalid load key, '\\x0c'.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÖŸÑŸÅ\n",
        "file_path = \"random_forest.pkl\"\n",
        "\n",
        "try:\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        rf_model = pickle.load(file)\n",
        "    print(\"‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ŸÜÿ¨ÿßÿ≠\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1FvxpE0ljxW",
        "outputId": "beba7ab9-9fcf-473f-fe2f-f3645e661e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ŸÜÿ¨ÿßÿ≠ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ joblib\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ joblib\n",
        "rf_model = joblib.load(\"random_forest.pkl\")\n",
        "\n",
        "print(\"‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ŸÜÿ¨ÿßÿ≠ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ joblib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6NnXFnF3mD38",
        "outputId": "92ea822e-2d46-4834-dac8-9f0021d49456"
      },
      "outputs": [
        {
          "ename": "UnpicklingError",
          "evalue": "invalid load key, '\\x0c'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f2def8fb0e34>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random_forest.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ÿßŸÜÿ™Ÿáÿßÿ° ÿ™ÿ≥ÿ¨ŸäŸÑ ÿßŸÑŸàŸÇÿ™\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x0c'."
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\n",
        "with open(\"random_forest.pkl\", \"rb\") as file:\n",
        "    rf_model = pickle.load(file)\n",
        "\n",
        "# ÿßŸÜÿ™Ÿáÿßÿ° ÿ™ÿ≥ÿ¨ŸäŸÑ ÿßŸÑŸàŸÇÿ™\n",
        "end_time = time.time()\n",
        "\n",
        "# ÿ≠ÿ≥ÿßÿ® ŸàŸÇÿ™ ÿßŸÑÿ™ÿ≠ŸÖŸäŸÑ\n",
        "loading_time = end_time - start_time\n",
        "print(loading_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVAaOzS6m82c",
        "outputId": "6f0b3ba4-03f9-4349-89a2-d6a5f6fc1e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ŸÜÿ¨ÿßÿ≠ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ joblib\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ joblib\n",
        "rf_model = joblib.load(\"random_forest.pkl\")\n",
        "\n",
        "print(\"‚úÖ ÿ™ŸÖ ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ®ŸÜÿ¨ÿßÿ≠ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ joblib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgPL2Zr8nRBM",
        "outputId": "3c973b51-810f-46c0-de26-3899ad31bb71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ÿ™ŸÖ ÿ™ŸÜŸÅŸäÿ∞ ÿßŸÑÿ™ŸÜÿ®ÿ§ ÿ®ŸÜÿ¨ÿßÿ≠!\n",
            "‚è±Ô∏è ÿßŸÑÿ≤ŸÖŸÜ ÿßŸÑŸÖÿ≥ÿ™ÿ∫ÿ±ŸÇ ŸÑŸÑÿ™ŸÜÿ®ÿ§: 0.006485 ÿ´ÿßŸÜŸäÿ©\n",
            "üîÆ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÖÿ™ŸàŸÇÿπÿ©: [1]\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨\n",
        "rf_model = joblib.load(\"random_forest.pkl\")\n",
        "\n",
        "# ÿ™ŸàŸÑŸäÿØ ÿ®ŸäÿßŸÜÿßÿ™ ÿßÿÆÿ™ÿ®ÿßÿ± ÿπÿ¥Ÿàÿßÿ¶Ÿäÿ© (ÿßÿ≥ÿ™ÿ®ÿØŸÑŸáÿß ÿ®ÿ®ŸäÿßŸÜÿßÿ™ŸÉ ÿßŸÑÿ≠ŸÇŸäŸÇŸäÿ©)\n",
        "X_test = np.random.rand(1, rf_model.n_features_in_)  # ÿπÿØÿØ ÿßŸÑŸÖŸäÿ≤ÿßÿ™ Ÿäÿ¨ÿ® ÿ£ŸÜ Ÿäÿ∑ÿßÿ®ŸÇ ŸÖÿß ÿ™ŸÖ ÿ™ÿØÿ±Ÿäÿ®Ÿá\n",
        "\n",
        "# ÿ™ÿ≥ÿ¨ŸäŸÑ ŸàŸÇÿ™ ÿßŸÑÿ®ÿØÿ°\n",
        "start_time = time.time()\n",
        "\n",
        "# ÿ™ŸÜŸÅŸäÿ∞ ÿßŸÑÿ™ŸÜÿ®ÿ§\n",
        "prediction = rf_model.predict(X_test)\n",
        "\n",
        "# ÿ™ÿ≥ÿ¨ŸäŸÑ ŸàŸÇÿ™ ÿßŸÑÿßŸÜÿ™Ÿáÿßÿ°\n",
        "end_time = time.time()\n",
        "\n",
        "# ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ≤ŸÖŸÜ ÿßŸÑŸÖÿ≥ÿ™ÿ∫ÿ±ŸÇ\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# ÿ∑ÿ®ÿßÿπÿ© ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨\n",
        "print(f\"‚úÖ ÿ™ŸÖ ÿ™ŸÜŸÅŸäÿ∞ ÿßŸÑÿ™ŸÜÿ®ÿ§ ÿ®ŸÜÿ¨ÿßÿ≠!\")\n",
        "print(f\"‚è±Ô∏è ÿßŸÑÿ≤ŸÖŸÜ ÿßŸÑŸÖÿ≥ÿ™ÿ∫ÿ±ŸÇ ŸÑŸÑÿ™ŸÜÿ®ÿ§: {elapsed_time:.6f} ÿ´ÿßŸÜŸäÿ©\")\n",
        "print(f\"üîÆ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÖÿ™ŸàŸÇÿπÿ©: {prediction}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zPFj92xoBwu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
